# Chemical Reaction Extraction from Text

## Pre-training: ChemBERT

### Data preparation
    cd chem-pretrain/
    python data/data.py \
        --annotation-file ${annotation_file_path} \
        --output-file ${unlabeled_data_path}

`${annotation_file_path}` is generated by the reaction annotation platform ([https://github.com/jiangfeng1124/ieturk](https://github.com/jiangfeng1124/ieturk))

### Train
1. Generate training data for masked language modeling:
`./pregenerate.sh ${unlabeled_data_path} ${output_dir}`
2. Run:
`./mlm.sh ${gpu_id}`

## Task: Product Extraction

### Data preparation

    cd product-extraction/
    python preprocessing/data.py \
        --annotation-file ${annotation_file_path} \
        --output-file ${pe_labeled_path}
    
    # make the train/test split
    python preprocessing/split.py \
        --input-file ${pe_labeled_path} \
        --output-dir ${output_dir}

### Train

    # ${pre_model_path} can be: bert-base-cased, or any pre-trained BERT models (i.e. BioBERT)
    ./train.sh ${pre_model_path} ${ft_model_path} ${gpu_id}
    ./eval.sh ${ft_model_path} ${gpu_id}

## Task: Reaction Role Recognition
### Data preparation

    cd role-recognition/
    python preprocessing/data.py \
        --annotation-file ${annotation_file_path} \
        --output-file ${rr_labeled_path}
    
    # make the train/test split
    python preprocessingn/split.py \
        --input-file ${rr_labeled_path} \
        --output-dir ${output_dir}

### Train

    # ${pre_model_path} can be: bert-base-cased, or any pre-trained BERT models (i.e. BioBERT)
    ./train.sh ${pre_model_path} ${ft_model_path} ${gpu_id}
    ./eval.sh ${ft_model_path} ${gpu_id}


### TODO:

- [ ] One product, multiple reactions
- [ ] ChemBert v3.0

